from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import NoSuchElementException
import time
import pandas as pd
import numpy as np
import re
import json
from datetime import datetime, timedelta
import awswrangler as wr
from awswrangler.exceptions import EmptyDataFrame
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urlparse, parse_qs
import os
import warnings
import re
import urllib.parse
import mysql.connector
from sqlalchemy import create_engine
import yt_dlp
from tqdm import tqdm


#ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì˜ìƒ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
warnings.filterwarnings('ignore')
src_path = os.path.dirname(os.path.abspath(__file__))

class LG_YOUTUBE_CRAWLER():
    def __init__(self):
        # # DB ì—°ê²° ì •ë³´ load
        with open(os.path.join(src_path, "..", "keys","db_keys.json"), "rb") as f:
            db_keys = json.load(f)

        self.user = db_keys["user"]
        self.password = db_keys["password"]
        self.host = db_keys["host"]
        self.db = db_keys["db"]
        self.schema = db_keys["schema"]
        self.port = db_keys["port"]
        
        self.engine = create_engine(
            f"mysql+pymysql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db}"
        )


        # ê²°ê³¼ ì €ì¥ ë°ì´í„°í”„ë ˆì„
        self.result_df = pd.DataFrame(columns = ["video_id","keyword","channel_id", "channel_nm","channel_url", "title","description","thumbnail","tags","categories", "duration_string", "duration", "original_url","language","upload_date","load_dttm","add_yn","channel_follower_count"])

        #ì—ëŸ¬ í…Œì´ë¸” 
        self.error_df =  pd.DataFrame(columns = ["stddt","video_id", "channel_id", "error_flag", "program_name"])

        #ì ì¬ ì‹œê°„
        self.load_dttm = datetime.now().strftime("%Y%m%d%H")
                

        #í‚¤ì›Œë“œ + DB ì ì¬ëœ ì˜ìƒ IDê°€ì ¸ì˜¤ê¸°
        self.fetch_video_id()
        

        

    #ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ 
    def search_youtube(self,keyword, max_results=200):
        # ë‚ ì§œ í•„í„° í¬í•¨ (2025-01-01 ì´í›„)
        query = f"ytsearch{max_results}:after:2025-01-01 {keyword}"
        ydl_opts = {'quiet': True,'no_warnings': True }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            search_result = ydl.extract_info(query, download=False)

        #print('ğŸ” ê²€ìƒ‰ëœ ì˜ìƒ ìˆ˜:', len(search_result['entries']))
        
        # ì˜ìƒ ì •ë³´ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
        video_data = []
        
        for entry in search_result['entries']:
            # uploader_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±„ë„ URL ë§Œë“¤ê¸°
            uploader_id = entry.get('uploader_id')
            channel_url = f"https://www.youtube.com/{uploader_id}" if uploader_id else 'ì •ë³´ ì—†ìŒ'
            
            # í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ
            video_info = {
                'video_id': entry.get('id'),
                'title': entry.get('title'),
                'thumbnail': entry.get('thumbnail'),
                'description': entry.get('description'),
                'channel_id': uploader_id,
                'channel_nm': entry.get('uploader'),
                'channel_url': channel_url,
                'categories': entry.get('categories', 'ì •ë³´ ì—†ìŒ'),
                'tags': entry.get('tags', 'ì •ë³´ ì—†ìŒ'),
                'upload_date': entry.get('upload_date', 'ì •ë³´ ì—†ìŒ'),
                'original_url': entry.get('original_url'),
                'duration_string': entry.get('duration_string', 'ì •ë³´ ì—†ìŒ'),
                'language': entry.get('language', 'ì •ë³´ ì—†ìŒ'),
                'view_cnt' :entry.get('view_count'),
                'is_advertised': entry.get('has_ads', False),  # or 'advertised'
                #'channel_follower_count': entry.get('uploader_count'),
                'channel_follower_count': entry.get('channel_follower_count') or entry.get('uploader_subscriber_count') or entry.get('uploader_count'),
            }
            
            # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            video_data.append(video_info)


        
        # DataFrame ìƒì„±
        temp_df = pd.DataFrame(video_data)

        #ì˜ìƒì´ shortsê°€ ì•„ë‹Œê±° í•„í„°
        temp_df = temp_df[~temp_df['original_url'].str.contains('shorts')]

        return temp_df
 



    #ì´ˆë‹¨ìœ„ë¡œ ë°”ê¾¸ê¸° í•¨ìˆ˜
    def convert_to_seconds(self,time_str):
        parts = list(map(int, time_str.strip().split(":")))
        
        if len(parts) == 3:  # hh:mm:ss
            hours, minutes, seconds = parts
        elif len(parts) == 2:  # mm:ss
            hours = 0
            minutes, seconds = parts
        elif len(parts) == 1:  # ss
            hours = 0
            minutes = 0
            seconds = parts[0]
        else:
            return 0  # ì˜ëª»ëœ í˜•ì‹ ì²˜ë¦¬

        return hours * 3600 + minutes * 60 + seconds

    #00:00:00í˜•ì‹ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def to_hhmmss(self,time_str):
        parts = time_str.strip().split(":")
        if len(parts) == 2:
            minutes, seconds = parts
            hours = 0
        elif len(parts) == 3:
            hours, minutes, seconds = parts
        else:
            return "00:00:00"  # ì˜ˆì™¸ ì²˜ë¦¬

        # 0 ì±„ì›Œì„œ hh:mm:ss í˜•ì‹ìœ¼ë¡œ
        return f"{int(hours):02}:{int(minutes):02}:{int(seconds):02}"

        

    # JSON ë³€í™˜ í•¨ìˆ˜ ì •ì˜
    def convert_to_json(self,thumbnail):
        data = {
            "url": thumbnail
        }
        return json.dumps(data)
    
    #ì˜ìƒ ì„¤ëª…ë¶€ë¶„ ì¶”ì¶œ
    def description_script(self,description):        
                
        if '#' in description :
            #ë“±ì¥í•˜ëŠ” ì• ë¶€ë¶„ê¹Œì§€ ì»¤íŠ¸
            description= description.split('#')[0]        
        
        return description
    
    #íƒœê·¸ì •ë³´ ë³‘í•©í•©
    def merge_tags(self,description, tags):
        # descriptionì—ì„œ í•´ì‹œíƒœê·¸ ì¶”ì¶œ
        hashtags = re.findall(r'#(\w+)', description)
        
        if tags is None:
            tags = []
        
        # ì¤‘ë³µ ì œê±° í›„ ë³‘í•©
        merged_tags = list(set(hashtags + tags))
        return merged_tags
    
    #ìœ ë¡œê´‘ê³ ì²´í¬í•¨ìˆ˜
    def is_paid_promotion(self,is_advertised,description):

        #ìœ ë¡œê´‘ê³  ë°°ë„ˆì´ë¯¸ì§€ í™•ì¸
        has_paid_content = False
        #ì„¤ëª… ë¶€ë¶„ì— ê´‘ê³  í™•ì¸
        has_ad = False


        # ê´‘ê³  í‚¤ì›Œë“œ
        ad_keywords = [
            "ìœ ë£Œ ê´‘ê³ ", "ê´‘ê³ ë¥¼ í¬í•¨", "ê´‘ê³  í¬í•¨", "ê´‘ê³  ì˜ìƒ",
            "ì œì‘ë¹„ ì§€ì›", "ì œì‘ë¹„ë¥¼ ì§€ì›", "ì œì‘ ì§€ì›", "ë¬´ìƒ ì œê³µ", "ì œí’ˆ ì œê³µ",
            "í˜‘ì°¬ë°›ì•„", "í˜‘ì°¬ë°›ì€", "í›„ì›", "ìŠ¤í°ì„œ", "ìŠ¤í°ì„œì‹­",
            "ì œíœ´", "ì œíœ´ ë§í¬", "í™ë³´", "ë¦¬ë·°ìš©", "ì²´í—˜ë‹¨",
            "ì´ ì˜ìƒì€", "ê´‘ê³ ì…ë‹ˆë‹¤", "ì œê³µë°›ì•˜ìŠµë‹ˆë‹¤", "ì§€ì›ë°›ì•˜ìŠµë‹ˆë‹¤",
            "PR ìƒ˜í”Œ", "í˜‘ì—…", "ë¸Œëœë“œ ì œê³µ", "ë¸Œëœë“œ ì§€ì›","ì œì‘ë¹„ë¥¼ ì§€ì›ë°›ì•„","ìˆ˜ìˆ˜ë£Œë¥¼"
        ]

        # ë¶€ì • í‚¤ì›Œë“œ
        negation_keywords = [
            "ë‚´ëˆë‚´ì‚°","í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤", "í¬í•¨í•˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤", "ê´‘ê³ ê°€ ì•„ë‹™ë‹ˆë‹¤","ê´‘ê³  ì•„ë‹™ë‹ˆë‹¤", "ìœ ë£Œê´‘ê³ ê°€ ì•„ë‹™ë‹ˆë‹¤",
            "í˜‘ì°¬ì´ ì•„ë‹˜","í˜‘ì°¬ ì•„ë‹™", "ë¬´ìƒ ì œê³µ ì•„ë‹˜", "ê´‘ê³  ì•„ë‹˜", "ìŠ¤í°ì„œ ì•„ë‹˜",
            "ê´‘ê³ ë¥¼ í¬í•¨í•˜ì§€", "ê´‘ê³  ì—†ìŒ", "ìŠ¤í°ì„œì‹­ì´ ì•„ë‹™ë‹ˆë‹¤", "ì œê³µë°›ì§€ ì•ŠìŒ","ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤"
        ]
        

        # 1. ê´‘ê³  ë°°ë„ˆê°€ ìˆë‹¤ë©´ ê´‘ê³  í¬í•¨
        if is_advertised:
            has_paid_content = True

        # 2.ì˜ìƒ ì„¤ëª… ë¶€ë¶„ì— ê´‘ê³  ë¬¸êµ¬ í™•ì¸.
        if not any(neg in description for neg in negation_keywords):
            if any(ad in description for ad in ad_keywords):
                has_ad = True



        #í•˜ë‚˜ë¼ë„ í¬í•¨ëœë‹¤ë©´ , ê´‘ê³  
        has_any_ad = has_paid_content or has_ad
        if has_any_ad:
            
            has_any_ad='Y'
        else:
            
            has_any_ad='N'

        
        return has_any_ad              
    
    #ì‹¤í–‰ í•¨ìˆ˜
    def youtube_crawler_runner(self):
        #ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        desired_columns = [
                        "video_id", "keyword", "channel_id", "channel_nm", "channel_url",
                        "title", "description", "thumbnail", "tags", "categories",
                        "duration_string", "duration", "original_url", "language",
                        "upload_date", "load_dttm","add_yn","channel_follower_count"
                    ]
        try:
            for k in tqdm(range(len(self.input_keyword_df))) :
                input_keyword = self.input_keyword_df.loc[k, 'keyword']
                check_yn = self.input_keyword_df.loc[k, 'del_yn']

                #í‚¤ì›Œë“œ í…Œì´ë¸”ì—ì„œ ì‚­ì œì²˜ë¦¬ ì•ˆëœ í‚¤ì›Œë“œ ì§„í–‰ 
                if check_yn =='N':                 
                    
                    #DBì— ì ì¬ëœ í‚¤ì›Œë“œë¡œ ë“±ë¡ëœ ì˜ìƒë“¤ ì¡°íšŒ
                    temp_df = self.search_youtube(input_keyword,max_results=200)
                                                                                                                                                                                                     
                    # ì¡°íšŒìˆ˜ í•„í„°ë§(100íšŒì´ìƒìƒ)
                    print('ìˆ˜ì§‘ ì˜ìƒ ìˆ˜:',len(temp_df))
                    temp_df = temp_df[temp_df['view_cnt']>100].reset_index(drop=True)    
                    temp_df = temp_df.drop(columns=['view_cnt'])

                    # ì¤‘ë³µë˜ëŠ” ê°’ ì‚­ì œ
                    temp_df = temp_df.drop_duplicates(subset='video_id', keep='first')
                    print('ì „ì²˜ë¦¬ í›„ ì˜ìƒìˆ˜:',len(temp_df))

                    # í‚¤ì›Œë“œ ,ì ì¬ì¼ì,ì˜ìƒê¸¸ì´(ì´ˆ) ì»¬ëŸ¼ ì¶”ê°€
                    temp_df['keyword'] = input_keyword
                    temp_df['load_dttm'] = self.load_dttm
                    
                    
                    #ë°ì´í„° ì „ì²˜ë¦¬
                    # thumbnail JSON ë³€í™˜
                    temp_df['thumbnail'] = temp_df['thumbnail'].apply(self.convert_to_json)

                    #ì˜ìƒ ì„¤ëª… ì „ì²˜ë¦¬
                    temp_df['description'] = temp_df['description'].apply(self.description_script)

                    #íƒœê·¸ì •ë³´ ë³‘í•©
                    temp_df['tags'] = temp_df.apply(lambda row: self.merge_tags(row['description'], row['tags']), axis=1)
                    temp_df['tags'] = temp_df['tags'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)

                    #ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í˜• ë³€í™˜
                    temp_df['categories'] = temp_df['categories'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)


                    #ì˜ìƒê¸¸ì´ í˜•ë³€í™˜
                    temp_df['duration_string'] = temp_df['duration_string'].apply(self.to_hhmmss)
                    temp_df['duration'] = temp_df['duration_string'].apply(self.convert_to_seconds)

                    #ìœ ë¡œê´‘ê³  ì ìš©í•¨ìˆ˜
                    temp_df['add_yn'] = temp_df.apply(
                        lambda row: self.is_paid_promotion(row['is_advertised'], row['description']),
                        axis=1
                    )

                    # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
                    temp_df = temp_df[desired_columns]

                    self.result_df = pd.concat([self.result_df, temp_df], ignore_index=True)
        

        #ì—ëŸ¬ë°œìƒì‹œ ì—ëŸ¬ ë¡œê·¸ ì¶”ê°€           
        except Exception as err_msg:
                error_msg = str(err_msg)                
                self.error_df.loc[len(self.error_df)] = {
                    'stddt': self.load_dttm,
                    'video_id': None,
                    'channel_id': None,
                    'error_flag': 'Y',
                    'program_name': 'youtube_video_m',
                    'log_msg' : error_msg
                }
                #ì—ëŸ¬ ë¡œê·¸ ì¶”ê°€
                self.db_saver('error_log',self.error_df)    
                return

        #í”„ë¡œê·¸ë¨ì´ ì •ìƒ ì‘ë™ í–ˆì„ë•Œ ì•„ë˜ ì‹¤í–‰í–‰
        # #DBì— ì €ì¥ë˜ì–´ ìˆëŠ” ì˜ìƒ ID ì¡°íšŒ.
        # self.fetch_video_id()
        finally:
            # video_id ê¸°ì¤€ìœ¼ë¡œ self.keyword_dfì— í¬í•¨ë˜ì§€ ì•Šì€ ê²ƒë§Œ í•„í„°ë§
            self.result_df = self.result_df[~self.result_df['video_id'].isin(self.video_id_df['video_id'])].reset_index(drop=True)
            
            new_cnt = self.result_df.shape[0]
            print(f'ì‹ ê·œë¡œ ì¶”ê°€ëœ ì˜ìƒì€ "{new_cnt}"ê°œ ì…ë‹ˆë‹¤')

            error_msg = None
            #ì—ëŸ¬í…Œì´ë¸” ì—…ë°ì´íŠ¸
            self.error_df.loc[len(self.error_df)] = {
                'stddt': self.load_dttm,
                'video_id': None,
                'channel_id': None,
                'error_flag': 'N',
                'program_name': 'youtube_video_m',
                'log_msg' : error_msg
            }
            if new_cnt > 0:
                self.db_saver('youtube_video_m_temp',self.result_df)            
            else:
                print('ì¶”ê°€ë  ì˜ìƒ ì—†ìŠµë‹ˆë‹¤.')

            self.db_saver('error_log',self.error_df)            
   
    def db_connector(self):
        connection_url = f"mysql+pymysql://{self.user}:{self.password}@{self.host}:{self.port}/{self.db}"
        conn = create_engine(connection_url)
        return conn

    def db_saver(self, target_db_table, dataframe):
        # DB ì—°ê²° ê°ì²´
        conn = self.db_connector()
        # DB ë°ì´í„° ì ì¬
        dataframe.to_sql(
            name=target_db_table,
            con=conn,
            schema=self.schema,
            if_exists="append",
            index=False
        )
        conn.dispose()

    #ì˜ìƒë§ˆìŠ¤í„°ì˜ ê¸°ì¡´ ì˜ìƒIDê°€ì ¸ì˜¤ê¸° + í‚¤ì›Œë“œ í…Œì´ë¸” ê°’ ê°€ì ¸ì˜¤ê¸°
    def fetch_video_id(self):
        conn = self.db_connector()  # SQLAlchemy engine ë˜ëŠ” connection
        query = f"""
            SELECT DISTINCT video_id 
            FROM `{self.schema}`.youtube_video_m
        """
        self.video_id_df = pd.read_sql(query, con=conn)

        # í‚¤ì›Œë“œ ì¡°íšŒ
        query_keyword = f"""
            SELECT keyword, del_yn 
            FROM `{self.schema}`.input_keyword
        """
        self.input_keyword_df = pd.read_sql(query_keyword, con=conn)
       
        

    def log_writter(self, msg):
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_content = "[%s topx/naver_api_topx.py] %s  "%(current_time, msg)
        print(log_content)  

if __name__ == "__main__":
    topxc = LG_YOUTUBE_CRAWLER()

    start_time = time.time()
    topxc.youtube_crawler_runner()
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"\nâ± ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
